{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dcbc72cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\dell\\anaconda3\\anaconda_ai\\lib\\site-packages (4.24.0)\n",
      "Requirement already satisfied: torch in c:\\users\\dell\\anaconda3\\anaconda_ai\\lib\\site-packages (1.12.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\dell\\anaconda3\\anaconda_ai\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\dell\\anaconda3\\anaconda_ai\\lib\\site-packages (1.23.5)\n",
      "Requirement already satisfied: nltk in c:\\users\\dell\\anaconda3\\anaconda_ai\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\dell\\anaconda3\\anaconda_ai\\lib\\site-packages (1.2.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dell\\anaconda3\\anaconda_ai\\lib\\site-packages (from transformers) (22.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\dell\\anaconda3\\anaconda_ai\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in c:\\users\\dell\\anaconda3\\anaconda_ai\\lib\\site-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\dell\\anaconda3\\anaconda_ai\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\dell\\anaconda3\\anaconda_ai\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\dell\\anaconda3\\anaconda_ai\\lib\\site-packages (from transformers) (0.11.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\dell\\anaconda3\\anaconda_ai\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\dell\\anaconda3\\anaconda_ai\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\dell\\anaconda3\\anaconda_ai\\lib\\site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\dell\\anaconda3\\anaconda_ai\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dell\\anaconda3\\anaconda_ai\\lib\\site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\dell\\anaconda3\\anaconda_ai\\lib\\site-packages (from nltk) (1.1.1)\n",
      "Requirement already satisfied: click in c:\\users\\dell\\anaconda3\\anaconda_ai\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\dell\\anaconda3\\anaconda_ai\\lib\\site-packages (from scikit-learn) (1.10.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\dell\\anaconda3\\anaconda_ai\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dell\\anaconda3\\anaconda_ai\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\anaconda3\\anaconda_ai\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\dell\\anaconda3\\anaconda_ai\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\anaconda3\\anaconda_ai\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\anaconda3\\anaconda_ai\\lib\\site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\dell\\anaconda3\\anaconda_ai\\lib\\site-packages (from requests->transformers) (1.26.14)\n"
     ]
    }
   ],
   "source": [
    "# this is transformer model from hugging face library, GPT2 specifically, this model is basically trained on two different articles on Artificial intelligence which encompasses education and economy.\n",
    "# install these libraries and tools \n",
    "!pip install transformers torch pandas numpy nltk scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7ba00e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import these required libararies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4bb5a9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# this nltk library is crucial for text preprocessing it will remove all unnecessary marks within the data\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d919d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess text data\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove digits\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = ' '.join(word for word in text.split() if word not in stop_words)  # Remove stopwords\n",
    "    return text\n",
    "\n",
    "def load_and_preprocess_text(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        raw_text = file.read()\n",
    "    cleaned_text = preprocess_text(raw_text)\n",
    "    return cleaned_text\n",
    "\n",
    "file_path = 'Role of AI in education.txt'\n",
    "cleaned_text = load_and_preprocess_text(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d395abc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70bc14712672431da13c1c3cdf734da7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\anaconda3\\anaconda_AI\\lib\\site-packages\\huggingface_hub\\file_download.py:123: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\DELL\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd9a0dc3ccc4451682af676814f4e92e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "546ed4bbc12443a48e841b0659d9e125",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f88c6fb03b484c968f0cf39cf6524b40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\anaconda3\\anaconda_AI\\lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:54: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# did preparation of dataset for GPT-2 model\n",
    "\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "\n",
    "def save_cleaned_text_to_file(text, file_path):\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(text)\n",
    "\n",
    "cleaned_file_path = 'cleaned_role_of_ai_in_education.txt'\n",
    "save_cleaned_text_to_file(cleaned_text, cleaned_file_path)\n",
    "\n",
    "def load_dataset(file_path, tokenizer, block_size=512):\n",
    "    return TextDataset(\n",
    "        tokenizer=tokenizer,\n",
    "        file_path=file_path,\n",
    "        block_size=block_size\n",
    "    )\n",
    "\n",
    "def load_data_collator(tokenizer):\n",
    "    return DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False\n",
    "    )\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "dataset = load_dataset(cleaned_file_path, tokenizer)\n",
    "data_collator = load_data_collator(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9de26a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b8daf8b00bb43c88219e4d53486a8a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\anaconda3\\anaconda_AI\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1\n",
      "  Number of trainable parameters = 124439808\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:00, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1, training_loss=4.994874954223633, metrics={'train_runtime': 808.7292, 'train_samples_per_second': 0.004, 'train_steps_per_second': 0.001, 'total_flos': 783876096000.0, 'train_loss': 4.994874954223633, 'epoch': 1.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training and intiliazation of model\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e11eb219",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The role of AI in economy is to provide a better understanding of the economic and political context of the economy.\n",
      "\n",
      "The role of AI in economy is to provide a better understanding of the economic and political context of the economy.\n",
      "\n",
      "The role of AI in economy is to provide a better understanding of the economic and political context of the economy.\n",
      "\n",
      "The role of AI in economy is to provide a better understanding of the economic and political context of the economy.\n",
      "\n",
      "The role of AI\n"
     ]
    }
   ],
   "source": [
    "# generate any text on the based of trained dataset\n",
    "def generate_text(prompt, model, tokenizer, max_length=100):\n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    outputs = model.generate(inputs, max_length=max_length, num_return_sequences=1)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "prompt = \"The role of AI in economy\"\n",
    "generated_text = generate_text(prompt, model, tokenizer)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "de354864",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The role of education in the economy\n",
      "In this article, we will examine how a number factors affect economic growth. We also discuss why there is it important to look at educational attainment and what are some examples that can be taken into account when considering whether or not an individual should have access public school tuition assistance (PAS). In addition:\n"
     ]
    }
   ],
   "source": [
    "def generate_text(prompt, model, tokenizer, max_length=200, temperature=0.7, top_k=50, top_p=0.9, repetition_penalty=1.2):\n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    outputs = model.generate(\n",
    "        inputs,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        temperature=temperature,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        no_repeat_ngram_size=2  # Prevents repeating n-grams of specified length\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "prompt = \"The role of education\"\n",
    "generated_text = generate_text(prompt, model, tokenizer, max_length=200)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfc59ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
